---
title: "Loan Approval"
author: "Trillyon Earl"
date: "2025-03-01"
header-includes:
  - \usepackage{float}
  - \usepackage{booktabs}
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
```
```{r}
library(MASS)
library(car)
library(glmnet)
library(nnet)
library(leaps)
library(dplyr)
library(tidyverse)
library(Sleuth3)
library(logistf)
library(brglm2)
```

## Abstract
The data set chosen for this report was [Loan Approval Classification](https://www.kaggle.com/datasets/taweilo/loan-approval-classification-data). There are 14 predictor variables and 45000 observations. These predictor variables are:
\begin{table}
\centering
\begin{tabular}{l |c c} 
  Predictors & Type 
  \\ \hline
  Age & Float 
  \\ 
  Gender & Categorical
  \\ 
  Education & Categorical
  \\ 
  Income & Float
  \\ 
  Employment Years & Integer
  \\ 
  Home Ownership & Categorical
  \\ 
  Loan Amount & Float
  \\ 
  Loan Intent & Categorical
  \\ 
  Loan Interest Rate & Float
  \\ 
  Loan Percent Income & Float
  \\
  Credit History Length & Float
  \\ 
  Credit Score & Integer
  \\ 
  Previous Loans & Categorical
  \\
  Loan Status & Integer
\end{tabular}
\end{table} 

# Objective
The objective of this analysis is to accurately predict whether or not a person given some information about them would be approved for a loan. A value of 0 indicates rejecton and a values of 1 indicates acceptance for a loan in loan status.

# Data Clean Up 
```{r}
loan <- read.csv("./loan_data.csv")
loan <- as_tibble(loan)
loan <- loan |> mutate(
  person_gender = factor(person_gender,
    ordered = FALSE, levels = c("female", "male")
  ),
  person_education = factor(person_education,
    ordered = TRUE,
    levels = c("High School", "Associate", "Bachelor", "Master", "Doctorate")
  ),
  person_home_ownership = factor(person_home_ownership,
    ordered = FALSE,
    c("RENT", "OWN", "MORTGAGE", "OTHER")
  ),
  loan_intent = factor(loan_intent,
    ordered = FALSE,
    c(
      "PERSONAL", "MEDICAL", "EDUCATION",
      "VENTURE", "HOMEIMPROVEMENT", "DEBTCONSOLIDATION"
    )
  ),
  previous_loan_defaults_on_file = factor(previous_loan_defaults_on_file,
    ordered = FALSE, c("Yes", "No")
  ),
  person_income = scale(person_income),
  loan_amnt = scale(loan_amnt),
)
set.seed(123)
n <- nrow(loan) # 45 000
idx <- sample.int(n, size = 0.8 * n) # random 80 %
train <- loan[idx, ] # 36 000 rows
test <- loan[-idx, ] # 9 000 rows
test$loan_status <- as.numeric(as.character(test$loan_status))
train$loan_status <- as.numeric(as.character(train$loan_status))

predictors_numeric <- train |>
  select(where(is.numeric)) |>
  select(-loan_status)
```
```{r}
test_model <- function(model, testdat, cutoff = 0.5) {
  pred_prob <- predict(model, newdata = testdat, type = "response")
  pred_class <- ifelse(pred_prob > cutoff, 1, 0)
  data.frame(
    MSPE = mean((testdat$loan_status - predict(model, testdat, type = "response"))^2),
    Error = 1 - mean(pred_class == testdat$loan_status)
  )
}
```

# Methods
### Full Ordinary Logistic Model
This is the fitted model of all predictor variables that were left after data-clearning.
```{r}
full_model <- glm(loan_status ~ .,
  data = train,
  family = binomial(link = "logit")
)
summary(full_model)
test_model(full_model, test)
# Statistics
AIC(full_model)
BIC(full_model)
```

### Null Ordinary Logistic Model
This is the model of only the intercept.
```{r}
null_model <- glm(
  loan_status ~ 1,
  data   = train,
  family = binomial(link = "logit")
)

test_model(null_model, test)
anova(null_model, full_model, test = "Chisq")
AIC(null_model)
BIC(null_model)
```

### Null Model vs. Full Model
We can conclude from the statistics below that atleast one predictor variable is needed in our model as the we have gotten a very low p-value and the difference in AIC/BIC shows a very clear sign that we should not use the null model.

## Problematic Sample Points

### Leverage
We recieved around 5954 out of the 36000 oberservations that are considered high leverage.
```{r}
lev <- hatvalues(full_model)
cutoff <- (2 * length(coef(full_model))) / nrow(train)
lev_data <- data.frame(
  x_values = seq_along(lev),
  y_values = lev
)
ggplot(aes(x = x_values, y = y_values), data = lev_data) +
  geom_point() +
  geom_hline(yintercept = cutoff, col = "blue")
high_lev <- which(lev > cutoff)
head(high_lev)
length(high_lev)
```

### Outliers
We found 504 outliers within this data.
```{r}
rstd <- rstudent(full_model)
outliers <- which(abs(rstd) >= 1.97)
ggplot(aes(x = x_val, y = y_val), data = data.frame(x_val = seq_along(rstd), y_val = abs(rstd))) +
  geom_point() +
  geom_hline(yintercept = 1.97, col = "orange")
head(outliers)
length(outliers)
```

### Cook's Distance & Influential Points
Since 1 as the cutoff is too strict for this large dataset we adjusted the cutoff to be $\frac{4}{n}$ as used in a lot studies.
```{r}
cd <- cooks.distance(full_model)
influential <- which(cd > 4 / nrow(train))
influential <- unique(unlist(influential))
ggplot(aes(x = x_val, y = y_val), data = data.frame(x_val = seq_along(cd), y_val = cd)) +
  geom_point() +
  geom_hline(yintercept = 4 / nrow(train), col = "purple")
length(influential)
```

### Problematic Points
We remove outliers that are influential points. From the R code below we can see that all of our outliers are influential points thust we should remove each of them.
```{r}
sum(outliers %in% influential)
train_no_outlier <- train[-outliers, ]

no_outlier_model <- glm(loan_status ~ .,
  data = train_no_outlier,
  family = binomial(link = "logit")
)

summary(no_outlier_model)
test_model(no_outlier_model, test)
AIC(no_outlier_model)
BIC(no_outlier_model)
```

### Perfect and Quasi Complete Seperation
At this stage we have run into a problem. We encountered the warning message "glm.fit: fitted probabilities numerically 0 or 1 occurred". After researching this
we came across that this is because we have Quasi-Complete Seperation. This means that one or more of our predictor varibles almost perfectly predicts our outcome variable.previous_loan_defaults_on_fileNo  2
From anaylzing the summary of the full model we can spot multiple problem covariates. This is a problem because it can lead to overfitting, and it pushes the maximum likelihood and standard error towards infinity.

#### Number of Previous Loan Defaults On File
We can see that this covariate's estimate has a large magnitude around 20. This could be because of perfect and quasi complete seperation or multicollinearity.

### Combatting Perfect and Quesi-Complete Seperation
From our research a useful way to combat Quesi-Complete Seperation is to merge or drop factor levels with too few observations (< 0.5% of data size) or drop levels that are heavily sided towards one outcome. The reason for this is that
if these factor levels with few observations heavily lean towards one side of the binary outcome then when building a model it will see this almost perfect predictor for the outcome variable and thus
give us an esitmate with a very large magnitude for that factor level. Factor levels with too few observations also may have a wide confidence interval. Thus we collpase these factor levels into a factor level that accounts for a larger group of observations. Below is a count of how many times each
factor level occurs in the dataset of 36000 observations.
```{r}
table(train_no_outlier$loan_intent)
xtabs(~ loan_status + loan_intent, data = train_no_outlier)
table(train_no_outlier$person_education)
xtabs(~ loan_status + person_education, data = train_no_outlier)
table(train_no_outlier$person_home_ownership)
xtabs(~ loan_status + person_home_ownership, data = train_no_outlier)
table(train_no_outlier$previous_loan_defaults_on_file)
xtabs(~ loan_status + previous_loan_defaults_on_file, data = train_no_outlier)
```

We also see that we actually have complete seperation. Every observation that has had a previous loan default has been rejected for a loan.
We dont want to just remove this level because it still contains very important information thus we dedcided to combat this by using firth logistic regression as recommended in the source provided.

## Firth's Bias Reduced Logistic Regression

Firth logistic regression augments the ordinary log-likelihood with the
**Firth penalty**
\[
L_{\text{Firth}} = L_{\text{Logit}} \;-\; \frac{1}{2}\,\text{Penalty}
\]
where the penalty term is made to prevent the log-likelihood from becoming infinite in cases of seperation.

### Formula
\[
\frac{1}{2}\,\log\!\bigl\lvert I(\boldsymbol{\beta}) \bigr\rvert,
\]
where \(I(\boldsymbol{\beta})\) is the observed Fisher–information matrix.
```{r}
firth_model <- glm(
  formula = loan_status ~ ., data = train_no_outlier, family = "binomial",
  na.action = na.fail, method = "brglmFit"
)

summary(firth_model)
test_model(firth_model, test)
```
As shown in the summary of the Firth model there are still estimates with large magnitudes however the standard error for these estimates have decreased which tells us that it just has a strong effect on our data. For example the p value of previous loan defaults was large
because of it's standard error, however now it is near 0. However Firth Regression doesnt handle multi-collinearity like ridge regression thus we can still test for multi-collinearity which we will do utilizing VIF.

### Multi-Collinearity (VIF)
The two variables with deviances greater than 10 were age and employment experience. After comparing the deviance of both, removing employment experience resulted in a lower deviance thus we chose to remove that variable. After that removal there were no more VIF > 10.
```{r, results="hide"}
vif(firth_model)
deviance(firth_model)
firth_without_age <- glm(
  formula = loan_status ~ . - person_age, data = train_no_outlier, family = "binomial",
  na.action = na.fail, method = "brglmFit"
)
firth_without_emp_exp <- glm(
  formula = loan_status ~ . - person_emp_exp, data = train_no_outlier, family = "binomial",
  na.action = na.fail, method = "brglmFit"
)
vif(firth_without_emp_exp)
# STOP no more VIF less than 10 or 5
```

```{r}
deviance(firth_without_age)
deviance(firth_without_emp_exp)
summary(firth_without_emp_exp)
test_model(firth_without_emp_exp, test)
```

### Stepwise AIC Variable Selection on Firth Logistic Regression

#### Forward
Forward Stepwise AIC resulted in the original firth_model, no variabels were removed.
```{r}
AIC.f <- stepAIC(firth_model, direction = "forward", k = 2, trace = FALSE)
test_model(AIC.f, test)
```

#### Backward
Backward Stepwise AIC resulted in the removal of gender, education, and income variables.
Gender, Education, Income
```{r}
AIC.b <- stepAIC(firth_model, direction = "backward", k = 2, trace = FALSE)
test_model(AIC.b, test)
```

#### Both
Similar to Backward Stepwise AIC, Alternating Stepwise AIC resulted in the same removals as Backward Stepwise AIC.
```{r}
AIC.both <- stepAIC(firth_model, direction = "both", k = 2, trace = FALSE)
test_model(AIC.both, test)
```

## Ridge Logistic Regression
The goal of ridge regression is to exchange increased bias for decreased variance. Ridge regression takes the ordinary least squares method and adds a penalty to it which minimizes the sum of squared residuals (RSS) plus the penalty,
sum of coefficients (B_j)) squared times lambda, where lambda determines the severity of the penalty. This results in a smaller coefficients as we can in the below R code. 
In the case of Logistic Regression, Ridge Regression minimizes the sum of the likelihoods plus the sum of coefficients squared times lambda. But how do we choose a value of lambda that doesnt minimize and the coefficients too much but just enough?
We use Cross-Validation, specifically 10-fold-cross validation to determine which lambda results in the lowest SSE. Another useful thing about Ridge Regression is that we do not need to do any variable selection as Ridge Regression already minimizes coefficients that would be
removed from various variable selection methods.
```{r}
x_train <- model.matrix(loan_status ~ ., data = train_no_outlier)[, -1]
y_train <- train_no_outlier$loan_status

cv_ridge <- cv.glmnet(
  x_train, y_train,
  family = "binomial",
  alpha = 0,
  nfolds = 10,
  type.measure = "class"
)
plot(cv_ridge)

lambda_min <- cv_ridge$lambda.min # lambda that minimises CV error
lambda_1se <- cv_ridge$lambda.1se
lambda_min

ridge_model <- glmnet(
  x_train, y_train,
  family = "binomial",
  alpha = 0,
  lambda = lambda_min
)

coef(ridge_model)
x_train <- model.matrix(loan_status ~ ., data = train_no_outlier)[, -1]

x_test <- model.matrix(loan_status ~ ., data = test)[, -1]

x_test <- x_test[, colnames(x_train)]

p_hat <- predict(ridge_model,
  newx = x_test,
  s = "lambda.min",
  type = "response"
)[, 1]
pred_class <- ifelse(p_hat > 0.5, 1, 0)

data.frame(
  MSPE = mean((test$loan_status - p_hat)^2),
  Error = 1 - mean(pred_class == test$loan_status)
)
```

### Why Not LASSO
We decided to use Ridge regression instead of LASSO regression soley because of the fact we wanted to keep all of our coefficients even if they aren't impactful as they still provide some sense of
inference.

# Analysis & Results

The different regression methods we used were Ordinary Logistic Regression, Firth Logistic Regression, and Ridge Regression. We applied different variable selection techniques such as Stepwise AIC and Mallow Cp.

## Ordinary Logistic Regression
- The logistic full model (loan_status ~ .) with the original training data.
- The logistic null model (loan_status ~ 1) with the original training data and Drop-in Deviance test.
- The logistic model (loan_status ~ .) with the omitted outliers training data.

## Firth's Logistic Regression
- The firth logistic model (loan_status ~ . - person_emp_exp) with the omitted outliers training data 
- The firth logistic model (loan_status ~ . - person_emp_exp) with the omitted outliers training data and variable selection using VIF and StepwiseAIC


## Ridge Logistic Regression
- The Ridge logistic model (loan_status ~ .) with the ommitted outliers and with 10-fold cross-validation.  

## Results
To evaluate predictive performance we applied each fitted model to the 20% test data and recorded the mean-squared prediction error (MSPE) and the classification error rate at a 0.5 cut-off.
 
\begin{table}[htbp]
\centering
\begin{tabular}{lcc}
\toprule
\textbf{Model (training setup)}               & \textbf{MSPE}          & \textbf{Prediction Error Rate} \\
\midrule
\multicolumn{3}{l}{\em Ordinary Logistic Regression}\\
\quad Full model, original data                       & $0.07304491$     & $0.1061111$   \\
\quad Null model, Drop-in-Deviance test                & $0.1729012$      & $0.2223333$   \\
\quad Full model, outliers removed                     & $0.07450024$   & $0.1063333$\\[6pt]

\multicolumn{3}{l}{\em Firth’s Logistic Regression}\\
\quad Firth, outliers removed                                        & $0.07447477$   & $0.1063333$ \\
\quad Firth ($\!-\,$person\_emp\_exp), outliers removed                & $0.0745102$   & $0.1066667$ \\
\quad Firth + VIF + Stepwise AIC                                         & $0.07443141$   & $0.106$ \\[6pt]

\multicolumn{3}{l}{\em Ridge Logistic Regression}\\
\quad Ridge (10-fold CV, $\lambda_{\text{min}}$), outliers removed      & $0.0767909$         & $0.1051111$       \\
\bottomrule
\end{tabular}
\end{table}

# Conclusion

## Best-performing model
Based off of prediction error rate, which is the most important to us for this analysis, ridge regression produced the lowest error rate and thus was our best performing model.

## Handling separation
During the study we encountered both perfect and quasi-complete separation—most prominently for the “previous loan defaults” variable. Ordinary logistic regression produced huge, unstable coefficients and warnings. Firth’s bias-reduced logistic regression resolved this by adding the a penalty, giving us more interpretable estimates.

## Effect of variable selection
Stepwise AIC on the Firth model removed predictors that contributed little additional deviance, reducing model complexity without decreasing accuracy. Ridge regression, however, implicitly performs continuous shrinkage, retaining all variables and avoiding stepwise search.

## Practical insight
Separation and multicollinearity require different remedies. Firth correction fixes the former, while ridge regression tackles the latter. In our loan-approval data, we can assume multicollinearity was the stronger driver of error, thus ridge emerged as the overall winner.

# Sources
- https://stats.oarc.ucla.edu/other/mult-pkg/faq/general/faqwhat-is-complete-or-quasi-complete-separation-in-logistic-regression-and-what-are-some-strategies-to-deal-with-the-issue/
- https://medium.datadriveninvestor.com/firths-logistic-regression-classification-with-datasets-that-are-small-imbalanced-or-separated-49d7782a13f1

